def TIMER = "H H/3 * * *"
def NODE = "rhcos-jenkins"
def DOCKER_IMG = "quay.io/cgwalters/coreos-assembler"
def DOCKER_ARGS = "-v /srv:/srv --privileged --device /dev/kvm"
def API_CI_REGISTRY = "registry.svc.ci.openshift.org"
def OSCONTAINER_IMG = API_CI_REGISTRY + "/rhcos/os-maipo"

// Are there really other regions?  We're sending explorers to find out
def AWS_REGION = "us-east-1"

// this var conveniently refers to a location on the server as well as the
// local dir we sync to/from
def images = "/srv/rhcos/output/images"

def ref = "openshift/3.10/x86_64/os";

node(NODE) {
    def par_stages = [:]
    stage("Clean workspace") {
       step([$class: 'WsCleanup'])
    }
    checkout scm
    utils = load("pipeline-utils.groovy")
    utils.define_properties(TIMER)

    // Split the credentials up
    def username, password;
    withCredentials([
        usernameColonPassword(credentialsId: params.REGISTRY_CREDENTIALS, variable: 'CREDS'),
    ]) {
        (username, password) = "${CREDS}".split(':')
    }

    docker.image(DOCKER_IMG).pull()
    docker.image(DOCKER_IMG).inside(DOCKER_ARGS) {
    stage("Login") { sh """
        set +x
        echo podman login -u ${username} -p '<password>' ${API_CI_REGISTRY}
        podman login -u "${username}" -p "${password}" ${API_CI_REGISTRY}
        echo "login done"
    """ }

    stage("Clone qcow2-to-vagrant") {
        // I tried to use git() but not sure where that went, it isn't the workspace
        sh('git clone https://github.com/cgwalters/qcow2-to-vagrant')
    }

    stage("Sync In") {
        withCredentials([
            string(credentialsId: params.ARTIFACT_SERVER, variable: 'ARTIFACT_SERVER'),
            sshUserPrivateKey(credentialsId: params.ARTIFACT_SSH_CREDS_ID, keyFileVariable: 'KEY_FILE'),
        ]) {
            sh "mkdir -p ${images}"
            utils.rsync_dir_in(ARTIFACT_SERVER, KEY_FILE, images)
        }

        // Also initialize an ostree repo for change detection
        withCredentials([
            string(credentialsId: params.OSTREE_INSTALL_URL, variable: 'OSTREE_INSTALL_URL'),
        ]) {
            sh """
                rm -rf repo && ostree init --repo=repo --mode=bare-user
                ostree --repo=repo remote add rhcos --no-gpg-verify ${OSTREE_INSTALL_URL}
                ostree --repo=repo pull --mirror --commit-metadata-only rhcos
            """
        }
    }

    // Check if there's a new version out.
    // Really, we should be checksumming e.g. the ks and tdl too.
    def latest_commit, last_build_commit, latest_version, force_nocache
    stage("Check for Changes") {
        latest_commit = utils.sh_capture("ostree --repo=repo rev-parse ${ref}")
        latest_version = utils.get_rev_version("repo", "${latest_commit}")
        if (fileExists('force-nocache-build')) {
            force_nocache = readFile('force-nocache-build').trim();
        }
        last_build_commit = utils.sh_capture("cat ${images}/cloud/latest/commit.txt || :")
    }

    if (!params.DRY_RUN && (latest_commit == last_build_commit && latest_commit != force_nocache)) {
        echo "Last built ${latest_version} (${latest_commit}) - no changes"
        currentBuild.result = 'SUCCESS'
        currentBuild.description = '(No changes)'
        return
    }

    stage("Prepare Configs") {
        withCredentials([
            string(credentialsId: params.OSTREE_INSTALL_URL, variable: 'OSTREE_INSTALL_URL'),
        ]) {
            sh "sed -i 's,@@OSTREE_INSTALL_URL@@,${OSTREE_INSTALL_URL},' cloud.ks"
            sh "sed -i 's,@@OSTREE_INSTALL_REF@@,${ref},' cloud.ks"
        }
    }

    // Note this runs unprivileged to avoid the virt stack trying to use privileges
    // We run in bwrap which sets up a pid namespace +init to ensure qemu is reaped,
    // which libvirt relies on.
    def image = "/tmp/rhcos.qcow2"
    try {
    withCredentials([
        string(credentialsId: params.INSTALLER_TREE_URL, variable: 'INSTALLER_TREE_URL')]) {
    stage("Running coreos-virt-install") { sh """
        chmod a+rw /dev/kvm
        setfacl -m u:builder:rwX ${WORKSPACE}
        bwrap --bind / / --dev-bind /dev /dev --proc /proc --unshare-pid runuser -u builder -- coreos-virt-install --dest ${image} --create-disk \
                --console-log-file ${WORKSPACE}/rhcos-qcow2-install.txt \
                --wait=25 --kickstart cloud.ks --location "${INSTALLER_TREE_URL}"
        """
    } }
    } finally {
        archiveArtifacts artifacts: "rhcos-qcow2-install.txt", allowEmptyArchive: true
    }

    // Quick kola sanity test
    try {
    stage("Kola rhcos.basic") { sh """
        kola version
        kola -b rhcos --tapfile rhcos-basic.tap --qemu-image ${image} run rhcos.basic
    """ }
    } finally {
        archiveArtifacts artifacts: "rhcos-basic.tap", allowEmptyArchive: true
    }

    // We passed some sanity checks, so pull the image again, retag it, and push
    stage("Push an 'alpha' tag") {
        if (params.DRY_RUN) {
            echo "DRY_RUN set, skipping alpha tag"
        } else if (!params.DRY_RUN) { sh """
            podman pull ${OSCONTAINER_IMG}:${latest_commit}
            podman tag ${OSCONTAINER_IMG}:${latest_commit} ${OSCONTAINER_IMG}:alpha
            podman push ${OSCONTAINER_IMG}:alpha
        """ }
    }

    def dirname, img_prefix, commit, version, dirpath, qcow, vmdk, ec2, vagrant_libvirt
    stage("Postprocessing") {
        // just introspect after the fact to avoid race conditions
        commit = utils.sh_capture("""
            LIBGUESTFS_BACKEND=direct virt-cat -a ${image} -m /dev/coreos/root:/ \
                /ostree/repo/refs/remotes/rhcos/${ref}
        """)

        // do this again *after* running imgfac to close race
        sh "ostree --repo=repo pull --mirror --depth=0 --commit-metadata-only rhcos ${commit}"
        version = utils.get_rev_version("repo", commit)
        currentBuild.description = "${version} (${commit})"

        dirname = (new Date()).format("YYYY-MM-dd-HH-mm-ss-") + commit
        dirpath = "${images}/cloud/${dirname}"
        img_prefix = "${dirpath}/rhcos-${version}"
        qcow = "${img_prefix}.qcow2"
        vmdk = "${img_prefix}.vmdk"
        vagrant_libvirt = "${img_prefix}-vagrant-libvirt.box"
        sh "mkdir -p ${dirpath}"
        // this belongs better in a JSON file, but for now just use a file;
        // this is used higher up to determine no-op changes
        sh "echo '${commit}' > ${dirpath}/commit.txt"
        sh "mv ${image} ${qcow}"
    }
    // These three are generated from the QCOW2
    par_stages["vmdk"] = { -> stage("Generate vmdk") {
        sh """coreos-oemid ${qcow} ${vmdk}.tmp vmware
              qemu-img convert -f qcow2 -O vmdk ${vmdk}.tmp -o adapter_type=lsilogic,subformat=streamOptimized,compat6 ${vmdk}
              rm ${vmdk}.tmp"""
    } }
    // But note that EC2 goes into workspace, we upload it directly
    ec2 = "${WORKSPACE}/rhcos-${version}-aws.vmdk"
    par_stages["ec2"] = { -> stage("Generate EC2") {
        sh """coreos-oemid ${qcow} ${WORKSPACE}/rhcos-aws.qcow2 ec2
              qemu-img convert -f qcow2 -O vmdk ${WORKSPACE}/rhcos-aws.qcow2 -o adapter_type=lsilogic,subformat=streamOptimized,compat6 ${ec2}
              rm ${WORKSPACE}/rhcos-aws.qcow2"""
    } }
    // We use direct as we hit SELinux issues otherwise
    par_stages["vagrant-libvirt"] = { -> stage("Generate vagrant-libvirt") { sh """
        env LIBGUESTFS_BACKEND=direct ${WORKSPACE}/qcow2-to-vagrant/qcow2-to-vagrant ${qcow} ${vagrant_libvirt}
        ln -sr ${vagrant_libvirt} ${dirpath}/rhcos-vagrant-libvirt.box
        """
    } }
    par_stages["openstack"] = { -> stage("Generate OpenStack") {
        def img_openstack = "${img_prefix}-openstack.qcow2"
        sh """coreos-oemid ${qcow} ${img_openstack} openstack
              gzip ${img_openstack}
              ln -sr ${img_openstack}.gz ${dirpath}/rhcos-openstack.qcow2.gz
              """
    } }
    // Execute parallel group
    parallel par_stages; par_stages = [:]

    stage("Finalizing, generate metadata") {
        sh "gzip < ${qcow} > ${img_prefix}-qemu.qcow2.gz"
        sh "ln -sr ${img_prefix}-qemu.qcow2.gz ${dirpath}/rhcos-qemu.qcow2.gz"
        // Everything above in parallel worked on qcow, we're done with it now
        sh "rm ${qcow}"
        sh "ln -sfn ${dirname} ${images}/cloud/latest"
        // just keep the last 2 (+ latest symlink)
        sh "cd ${images}/cloud && (ls | head -n -3 | xargs -r rm -rf)"
        sh """
            rpm-ostree db list --repo=repo ${commit} > /${dirpath}/pkglist.txt
            cd ${dirpath} && find . -name '*.gz' -o -name '*.box' -o -name '*.vmdk' | \
            while read f; do
                test -f \${f}
                sha256sum \${f} | awk '{print \$1}' > \${f}.sha256sum
            done
        """
    }

   if (params.DRY_RUN) {
       echo "DRY_RUN set, skipping push"
       currentBuild.result = 'SUCCESS'
       currentBuild.description = '(dry run)'
       return
   }

    stage("Create AMI") {
            withCredentials([
                [$class: 'AmazonWebServicesCredentialsBinding', credentialsId: params.AWS_CREDENTIALS],
                string(credentialsId: params.S3_PRIVATE_BUCKET, variable: 'S3_PRIVATE_BUCKET'),
                string(credentialsId: params.AWS_CI_ACCOUNT, variable: 'AWS_CI_ACCOUNT'),
            ]) {
                sh """
                    # use a symlink so that our uploaded filename is unique
                    amijson=${dirpath}/aws-${AWS_REGION}.json
                    ore aws upload --region ${AWS_REGION} \
                        --ami-name 'rhcos_dev_${commit[0..6]}' \
                        --ami-description 'Red Hat CoreOS ${version} (${commit})' \
                        --bucket 's3://${S3_PRIVATE_BUCKET}/rhcos/cloud' \
                        --file ${ec2} \
                        --name "rhcos_dev_${commit[0..6]}" \
                        --delete-object | tee \${amijson}
                    rm ${ec2}

                    export AWS_DEFAULT_REGION=${AWS_REGION}

                    # Add the version and commit as tags to both the AMI and the underlying snapshot.
                    # We should teach mantle to apply extra tags.
                    ami=\$(jq -r .HVM \${amijson})
                    snapshot=\$(jq -r .SnapshotID \${amijson})
                    aws ec2 create-tags \
                        --resources \${ami} \${snapshot} \
                        --tags Key=ostree_commit,Value=${commit} \
                               Key=ostree_version,Value=${version}

                    # give launch permissions to OpenShift CI
                    aws ec2 modify-image-attribute \
                        --image-id \${ami} \
                        --launch-permission '{"Add":[{"UserId":"${AWS_CI_ACCOUNT}"}]}'
                """
            }
    }

    par_stages["rsync-out"] = { -> stage("rsync out") {
        withCredentials([
            string(credentialsId: params.ARTIFACT_SERVER, variable: 'ARTIFACT_SERVER'),
            sshUserPrivateKey(credentialsId: params.ARTIFACT_SSH_CREDS_ID, keyFileVariable: 'KEY_FILE'),
        ]) {
            utils.rsync_dir_out(ARTIFACT_SERVER, KEY_FILE, images)
        }
    } }
    par_stages["s3-upload"] = { -> stage("S3 upload") {
            withCredentials([
                [$class: 'AmazonWebServicesCredentialsBinding', credentialsId: params.AWS_CREDENTIALS],
                string(credentialsId: params.S3_PRIVATE_BUCKET, variable: 'S3_PRIVATE_BUCKET'),
                string(credentialsId: params.AWS_CI_ACCOUNT, variable: 'AWS_CI_ACCOUNT'),
            ]) { sh """
                aws s3 sync --delete ${images}/ s3://rhcos/images/
            """ }
        }
    }
    parallel par_stages; par_stages = [:]
    }
}
