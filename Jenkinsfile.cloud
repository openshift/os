def TIMER = "H H/3 * * *"
def NODE = "rhcos-imgfac-jslave"
def DOCKER_IMG = "quay.io/cgwalters/coreos-assembler"
def DOCKER_ARGS = "--net=host -v /srv:/srv --privileged"

// Are there really other regions?  We're sending explorers to find out
def AWS_REGION = "us-east-1"

// this var conveniently refers to a location on the server as well as the
// local dir we sync to/from
def images = "/srv/rhcos/output/images"

def ref = "openshift/3.10/x86_64/os";

node(NODE) {
    docker.image(DOCKER_IMG).pull()

    stage("Clean workspace") {
       step([$class: 'WsCleanup'])
    }

    checkout scm
    utils = load("pipeline-utils.groovy")
    utils.define_properties(TIMER)

    def par_stages = [:]
    par_stages["provision"] = { -> stage("Provision") {
        sh """
           rpm -q imagefactory-plugins-TinMan \
                  libguestfs-tools-c \
                  rpm-ostree \
                  rsync || \
               dnf -y install ostree \
                              imagefactory-plugins-TinMan \
                              libguestfs-tools-c \
                              rpm-ostree \
                              rsync
           """
    } }
    par_stages["clone qcow2-to-vagrant"] = { -> stage("Clone qcow2-to-vagrant") {
        // I tried to use git() but not sure where that went, it isn't the workspace
        sh('git clone https://github.com/cgwalters/qcow2-to-vagrant')
    } }
    parallel par_stages; par_stages = [:]

    stage("Sync In") {
        withCredentials([
            string(credentialsId: params.ARTIFACT_SERVER, variable: 'ARTIFACT_SERVER'),
            sshUserPrivateKey(credentialsId: params.ARTIFACT_SSH_CREDS_ID, keyFileVariable: 'KEY_FILE'),
        ]) {
            utils.rsync_dir_in(ARTIFACT_SERVER, KEY_FILE, images)
        }

        // Also initialize an ostree repo for change detection
        withCredentials([
            string(credentialsId: params.OSTREE_INSTALL_URL, variable: 'OSTREE_INSTALL_URL'),
        ]) {
            sh """
                rm -rf repo && ostree init --repo=repo --mode=bare-user
                ostree --repo=repo remote add rhcos --no-gpg-verify ${OSTREE_INSTALL_URL}
                ostree --repo=repo pull --mirror --commit-metadata-only rhcos
            """
        }
    }

    // Check if there's a new version out.
    // Really, we should be checksumming e.g. the ks and tdl too.
    def latest_commit, latest_version, last_build_version, force_nocache
    stage("Check for Changes") {
        latest_commit = utils.sh_capture("ostree --repo=repo rev-parse ${ref}")
        latest_version = utils.get_rev_version("repo", "${latest_commit}")
        if (fileExists('force-nocache-build')) {
            force_nocache = readFile('force-nocache-build').trim();
        }
        last_build_version = utils.sh_capture("cat ${images}/cloud/latest/version.txt || :")
    }

    if (latest_version == last_build_version && latest_version != force_nocache) {
        echo "Last built ${latest_version} (${latest_commit}) - no changes"
        currentBuild.result = 'SUCCESS'
        currentBuild.description = '(No changes)'
        return
    }

    stage("Prepare Configs") {
        withCredentials([
            string(credentialsId: params.OSTREE_INSTALL_URL, variable: 'OSTREE_INSTALL_URL'),
            string(credentialsId: params.INSTALLER_TREE_URL, variable: 'INSTALLER_TREE_URL'),
        ]) {
            sh "sed -i 's,\\(<url>\\).*\\(<\\/url\\),\\1${INSTALLER_TREE_URL}\\2,' rhcos.tdl"
            sh "sed -i 's,@@OSTREE_INSTALL_URL@@,${OSTREE_INSTALL_URL},' cloud.ks"
            sh "sed -i 's,@@OSTREE_INSTALL_REF@@,${ref},' cloud.ks"
            sh "rm -rf /var/lib/imagefactory/storage/*"
        }
    }

    stage("Running imagefactory") {
        sh """
            imagefactory --debug base_image \
                --file-parameter install_script cloud.ks \
                --parameter offline_icicle True \
                rhcos.tdl
        """
    }

    def dirname, image, commit, version, dirpath, qcow, vmdk, vagrant_libvirt
    stage("Postprocessing") {
        image = utils.sh_capture("ls /var/lib/imagefactory/storage/*.body")
        // just introspect after the fact to avoid race conditions
        commit = utils.sh_capture("""
            LIBGUESTFS_BACKEND=direct virt-cat -a ${image} -m /dev/coreos/root:/ \
                /ostree/repo/refs/remotes/rhcos/${ref}
        """)

        // do this again *after* running imgfac to close race
        // fetch parent too for the later db diff
        sh "ostree --repo=repo pull --mirror --depth=1 --commit-metadata-only rhcos ${commit}"
        version = utils.get_rev_version("repo", commit)
        currentBuild.description = "${version} (${commit})"

        dirname = (new Date()).format("YYYY-MM-dd-HH-mm-ss-") + commit
        dirpath = "${images}/cloud/${dirname}"
        qcow = "${dirpath}/rhcos.qcow2"
        vmdk = "${dirpath}/rhcos.vmdk"
        vagrant_libvirt = "${dirpath}/rhcos-vagrant-libvirt.box"
        sh "mkdir -p ${dirpath}"
        // this belongs better in a JSON file, but for now just use a file;
        // this is used higher up to determine no-op changes
        sh "echo '${version}' > ${dirpath}/version.txt"
        // And do the qcow2 conversion now
        sh "qemu-img convert -f raw -O qcow2 ${image} ${qcow}"
        sh "rm -f ${image}"
    }
    par_stages["vmdk"] = { -> stage("Generate vmdk") {
        sh """env LIBGUESTFS_BACKEND=direct ${WORKSPACE}/scripts/coreos-oemid ${qcow} ${vmdk}.tmp vmware
              qemu-img convert -f qcow2 -O vmdk ${vmdk}.tmp -o adapter_type=lsilogic,subformat=streamOptimized,compat6 ${vmdk}
              rm ${vmdk}.tmp"""
    } }
    par_stages["vagrant-libvirt"] = { -> stage("Generate vagrant-libvirt") {
        // We use direct as we hit SELinux issues otherwise
        sh "env LIBGUESTFS_BACKEND=direct ${WORKSPACE}/qcow2-to-vagrant/qcow2-to-vagrant ${qcow} ${vagrant_libvirt}"
    } }
    parallel par_stages; par_stages = [:]

    stage("Finalizing, generate metadata") {
        sh """env LIBGUESTFS_BACKEND=direct ${WORKSPACE}/scripts/coreos-oemid ${qcow} ${qcow}.openstack openstack
              mv ${qcow}.openstack ${qcow}
              gzip ${qcow}"""
        sh "ln -sfn ${dirname} ${images}/cloud/latest"
        // just keep the last 2 (+ latest symlink)
        sh "cd ${images}/cloud && (ls | head -n -3 | xargs -r rm -rf)"
        // we're stuck with an older host because imagefactory produces kernel
        // panics on newer hosts, so let's use a container to have access to a
        // newer rpm-ostree that understands the new pkglist metadata
        docker.image(DOCKER_IMG).inside(DOCKER_ARGS) {
            def prev_commit = utils.sh_capture("ostree --repo=repo rev-parse ${commit}^")

            sh """
                rpm-ostree db list --repo=repo ${commit} > /${dirpath}/pkglist.txt
                rpm-ostree db diff --repo=repo ${prev_commit} ${commit} > ${dirpath}/pkgdiff.txt
                sha256sum ${qcow}.gz | awk '{print \$1}' > ${qcow}.gz.sha256sum
                sha256sum ${vmdk} | awk '{print \$1}' > ${vmdk}.sha256sum
                sha256sum ${vagrant_libvirt} | awk '{print \$1}' > ${vagrant_libvirt}.sha256sum
            """
        }
    }

   if (params.DRY_RUN) {
       echo "DRY_RUN set, skipping push"
       currentBuild.result = 'SUCCESS'
       currentBuild.description = '(dry run)'
       return
   }

    stage("Create AMI") {
        sh """
            env LIBGUESTFS_BACKEND=direct ${WORKSPACE}/scripts/coreos-oemid ${images}/cloud/latest/rhcos.vmdk rhcos-aws-${commit}.vmdk ec2
        """
        docker.image(DOCKER_IMG).inside(DOCKER_ARGS) {
            withCredentials([
                [$class: 'AmazonWebServicesCredentialsBinding', credentialsId: params.AWS_CREDENTIALS],
                string(credentialsId: params.S3_PRIVATE_BUCKET, variable: 'S3_PRIVATE_BUCKET'),
            ]) {
                sh """
                    # use a symlink so that our uploaded filename is unique
                    amijson=${dirpath}/aws-${AWS_REGION}.json
                    ore aws upload --region ${AWS_REGION} \
                        --ami-name 'rhcos_dev_${commit[0..6]}' \
                        --ami-description 'Red Hat CoreOS ${version} (${commit})' \
                        --bucket 's3://${S3_PRIVATE_BUCKET}/rhcos/cloud' \
                        --file rhcos-aws-${commit}.vmdk \
                        --name "rhcos_dev_${commit[0..6]}" \
                        --delete-object | tee \${amijson}

                    # Add the version and commit as tags to both the AMI and the underlying snapshot.
                    # We should teach mantle to apply extra tags.
                    ami=\$(jq -r .HVM \${amijson})
                    snapshot=\$(jq -r .SnapshotID \${amijson})
                    AWS_DEFAULT_REGION=${AWS_REGION} aws ec2 create-tags \
                        --resources \${ami} \${snapshot} \
                        --tags Key=ostree_commit,Value=${commit} \
                               Key=ostree_version,Value=${version}
                """
            }
        }
    }

    stage("Sync Out") {
        withCredentials([
            string(credentialsId: params.ARTIFACT_SERVER, variable: 'ARTIFACT_SERVER'),
            sshUserPrivateKey(credentialsId: params.ARTIFACT_SSH_CREDS_ID, keyFileVariable: 'KEY_FILE'),
        ]) {
            utils.rsync_dir_out(ARTIFACT_SERVER, KEY_FILE, images)
        }
    }
}
