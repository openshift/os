def TIMER = "H H/3 * * *"
def NODE = "rhcos-imgfac-jslave"
def DOCKER_IMG = "quay.io/cgwalters/coreos-assembler"
def DOCKER_ARGS = "--net=host -v /srv:/srv --privileged"

// Are there really other regions?  We're sending explorers to find out
def AWS_REGION = "us-east-1"

// this var conveniently refers to a location on the server as well as the
// local dir we sync to/from
def images = "/srv/rhcos/output/images"

def ref = "openshift/3.10/x86_64/os";

node(NODE) {
    docker.image(DOCKER_IMG).pull()

    stage("Clean workspace") {
       step([$class: 'WsCleanup'])
    }

    checkout scm
    utils = load("pipeline-utils.groovy")
    utils.define_properties(TIMER)

    def par_stages = [:]
    par_stages["provision"] = { -> stage("Provision") {
        sh """
           pkgs="imagefactory-plugins-TinMan \
                 libguestfs-tools-c \
                 rpm-ostree \
                 rsync"
           if ! rpm -q \$pkgs 2>/dev/null; then
             dnf -y install \$pkgs
           fi
           """
    } }
    par_stages["clone qcow2-to-vagrant"] = { -> stage("Clone qcow2-to-vagrant") {
        // I tried to use git() but not sure where that went, it isn't the workspace
        sh('git clone https://github.com/cgwalters/qcow2-to-vagrant')
    } }
    parallel par_stages; par_stages = [:]

    stage("Sync In") {
        withCredentials([
            string(credentialsId: params.ARTIFACT_SERVER, variable: 'ARTIFACT_SERVER'),
            sshUserPrivateKey(credentialsId: params.ARTIFACT_SSH_CREDS_ID, keyFileVariable: 'KEY_FILE'),
        ]) {
            utils.rsync_dir_in(ARTIFACT_SERVER, KEY_FILE, images)
        }

        // Also initialize an ostree repo for change detection
        withCredentials([
            string(credentialsId: params.OSTREE_INSTALL_URL, variable: 'OSTREE_INSTALL_URL'),
        ]) {
            sh """
                rm -rf repo && ostree init --repo=repo --mode=bare-user
                ostree --repo=repo remote add rhcos --no-gpg-verify ${OSTREE_INSTALL_URL}
                ostree --repo=repo pull --mirror --commit-metadata-only rhcos
            """
        }
    }

    // Check if there's a new version out.
    // Really, we should be checksumming e.g. the ks and tdl too.
    def latest_commit, last_build_commit, latest_version, force_nocache
    stage("Check for Changes") {
        latest_commit = utils.sh_capture("ostree --repo=repo rev-parse ${ref}")
        latest_version = utils.get_rev_version("repo", "${latest_commit}")
        if (fileExists('force-nocache-build')) {
            force_nocache = readFile('force-nocache-build').trim();
        }
        last_build_commit = utils.sh_capture("cat ${images}/cloud/latest/commit.txt || :")
    }

    if (latest_commit == last_build_commit && latest_commit != force_nocache) {
        echo "Last built ${latest_version} (${latest_commit}) - no changes"
        currentBuild.result = 'SUCCESS'
        currentBuild.description = '(No changes)'
        return
    }

    stage("Prepare Configs") {
        withCredentials([
            string(credentialsId: params.OSTREE_INSTALL_URL, variable: 'OSTREE_INSTALL_URL'),
            string(credentialsId: params.INSTALLER_TREE_URL, variable: 'INSTALLER_TREE_URL'),
        ]) {
            sh "sed -i 's,\\(<url>\\).*\\(<\\/url\\),\\1${INSTALLER_TREE_URL}\\2,' rhcos.tdl"
            sh "sed -i 's,@@OSTREE_INSTALL_URL@@,${OSTREE_INSTALL_URL},' cloud.ks"
            sh "sed -i 's,@@OSTREE_INSTALL_REF@@,${ref},' cloud.ks"
            sh "rm -rf /var/lib/imagefactory/storage/*"
        }
    }

    stage("Running imagefactory") {
        sh """
            imagefactory --debug base_image \
                --file-parameter install_script cloud.ks \
                --parameter offline_icicle True \
                rhcos.tdl
        """
    }

    def dirname, image, commit, version, dirpath, qcow, vmdk, ec2, vagrant_libvirt
    stage("Postprocessing") {
        image = utils.sh_capture("ls /var/lib/imagefactory/storage/*.body")
        // just introspect after the fact to avoid race conditions
        commit = utils.sh_capture("""
            LIBGUESTFS_BACKEND=direct virt-cat -a ${image} -m /dev/coreos/root:/ \
                /ostree/repo/refs/remotes/rhcos/${ref}
        """)

        // do this again *after* running imgfac to close race
        sh "ostree --repo=repo pull --mirror --depth=0 --commit-metadata-only rhcos ${commit}"
        version = utils.get_rev_version("repo", commit)
        currentBuild.description = "${version} (${commit})"

        dirname = (new Date()).format("YYYY-MM-dd-HH-mm-ss-") + commit
        dirpath = "${images}/cloud/${dirname}"
        qcow = "${dirpath}/rhcos.qcow2"
        vmdk = "${dirpath}/rhcos.vmdk"
        vagrant_libvirt = "${dirpath}/rhcos-vagrant-libvirt.box"
        sh "mkdir -p ${dirpath}"
        // this belongs better in a JSON file, but for now just use a file;
        // this is used higher up to determine no-op changes
        sh "echo '${commit}' > ${dirpath}/commit.txt"
        // And do the qcow2 conversion now
        sh "qemu-img convert -f raw -O qcow2 ${image} ${qcow}"
        sh "rm ${image}"
    }
    // These three are generated from the QCOW2
    par_stages["vmdk"] = { -> stage("Generate vmdk") {
        sh """env LIBGUESTFS_BACKEND=direct ${WORKSPACE}/scripts/coreos-oemid ${qcow} ${vmdk}.tmp vmware
              qemu-img convert -f qcow2 -O vmdk ${vmdk}.tmp -o adapter_type=lsilogic,subformat=streamOptimized,compat6 ${vmdk}
              rm ${vmdk}.tmp"""
    } }
    // But note that EC2 goes into workspace, we upload it directly
    ec2 = "${WORKSPACE}/rhcos-aws-${commit}.vmdk"
    par_stages["ec2"] = { -> stage("Generate EC2") {
        sh """env LIBGUESTFS_BACKEND=direct ${WORKSPACE}/scripts/coreos-oemid ${qcow} ${WORKSPACE}/rhcos-aws.qcow2 ec2
              qemu-img convert -f qcow2 -O vmdk ${WORKSPACE}/rhcos-aws.qcow2 -o adapter_type=lsilogic,subformat=streamOptimized,compat6 ${ec2}
              rm ${WORKSPACE}/rhcos-aws.qcow2"""
    } }
    par_stages["vagrant-libvirt"] = { -> stage("Generate vagrant-libvirt") {
        // We use direct as we hit SELinux issues otherwise
        sh "env LIBGUESTFS_BACKEND=direct ${WORKSPACE}/qcow2-to-vagrant/qcow2-to-vagrant ${qcow} ${vagrant_libvirt}"
    } }
    par_stages["openstack"] = { -> stage("Generate OpenStack") {
        sh """env LIBGUESTFS_BACKEND=direct ${WORKSPACE}/scripts/coreos-oemid ${qcow} ${qcow}.openstack openstack
              gzip < ${qcow}.openstack > ${qcow}.openstack.gz
              rm ${qcow}.openstack"""
    } }
    // Execute parallel group
    parallel par_stages; par_stages = [:]

    stage("Finalizing, generate metadata") {
        sh "gzip < ${qcow} > ${qcow}.qemu.gz"
        // Everything above in parallel worked on qcow, we're done with it now
        sh "rm ${qcow}"
        sh "ln -sfn ${dirname} ${images}/cloud/latest"
        // just keep the last 2 (+ latest symlink)
        sh "cd ${images}/cloud && (ls | head -n -3 | xargs -r rm -rf)"
        // we're stuck with an older host because imagefactory produces kernel
        // panics on newer hosts, so let's use a container to have access to a
        // newer rpm-ostree that understands the new pkglist metadata
        docker.image(DOCKER_IMG).inside(DOCKER_ARGS) {
            sh """
                rpm-ostree db list --repo=repo ${commit} > /${dirpath}/pkglist.txt
                for f in ${qcow}.qemu.gz ${qcow}.openstack.gz ${vmdk} ${vagrant_libvirt}; do
                    test -f \${f}
                    sha256sum \${f} | awk '{print \$1}' > \${f}.sha256sum
                done
            """
        }
    }

   if (params.DRY_RUN) {
       echo "DRY_RUN set, skipping push"
       currentBuild.result = 'SUCCESS'
       currentBuild.description = '(dry run)'
       return
   }

    stage("Create AMI") {
        docker.image(DOCKER_IMG).inside(DOCKER_ARGS) {
            withCredentials([
                [$class: 'AmazonWebServicesCredentialsBinding', credentialsId: params.AWS_CREDENTIALS],
                string(credentialsId: params.S3_PRIVATE_BUCKET, variable: 'S3_PRIVATE_BUCKET'),
                string(credentialsId: params.AWS_CI_ACCOUNT, variable: 'AWS_CI_ACCOUNT'),
            ]) {
                sh """
                    # use a symlink so that our uploaded filename is unique
                    amijson=${dirpath}/aws-${AWS_REGION}.json
                    ore aws upload --region ${AWS_REGION} \
                        --ami-name 'rhcos_dev_${commit[0..6]}' \
                        --ami-description 'Red Hat CoreOS ${version} (${commit})' \
                        --bucket 's3://${S3_PRIVATE_BUCKET}/rhcos/cloud' \
                        --file ${ec2} \
                        --name "rhcos_dev_${commit[0..6]}" \
                        --delete-object | tee \${amijson}
                    rm ${ec2}

                    export AWS_DEFAULT_REGION=${AWS_REGION}

                    # Add the version and commit as tags to both the AMI and the underlying snapshot.
                    # We should teach mantle to apply extra tags.
                    ami=\$(jq -r .HVM \${amijson})
                    snapshot=\$(jq -r .SnapshotID \${amijson})
                    aws ec2 create-tags \
                        --resources \${ami} \${snapshot} \
                        --tags Key=ostree_commit,Value=${commit} \
                               Key=ostree_version,Value=${version}

                    # give launch permissions to OpenShift CI
                    aws ec2 modify-image-attribute \
                        --image-id \${ami} \
                        --launch-permission '{"Add":[{"UserId":"${AWS_CI_ACCOUNT}"}]}'
                """
            }
        }
    }

    par_stages["rsync-out"] = { -> stage("rsync out") {
        withCredentials([
            string(credentialsId: params.ARTIFACT_SERVER, variable: 'ARTIFACT_SERVER'),
            sshUserPrivateKey(credentialsId: params.ARTIFACT_SSH_CREDS_ID, keyFileVariable: 'KEY_FILE'),
        ]) {
            utils.rsync_dir_out(ARTIFACT_SERVER, KEY_FILE, images)
        }
    } }
    par_stages["s3-upload"] = { -> stage("S3 upload") {
        docker.image(DOCKER_IMG).inside(DOCKER_ARGS) {
            withCredentials([
                [$class: 'AmazonWebServicesCredentialsBinding', credentialsId: params.AWS_CREDENTIALS],
                string(credentialsId: params.S3_PRIVATE_BUCKET, variable: 'S3_PRIVATE_BUCKET'),
                string(credentialsId: params.AWS_CI_ACCOUNT, variable: 'AWS_CI_ACCOUNT'),
            ]) { sh """
                aws s3 sync --delete ${images}/ s3://rhcos/images/
            """ }
        }
    } }
    parallel par_stages; par_stages = [:]
}
